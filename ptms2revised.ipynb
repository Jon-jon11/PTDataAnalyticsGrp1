{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0a4f32-3029-4dc2-91fe-dd9c85c5f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FinMark Corporation: Robust Data Pipeline\n",
    "# **Enhanced with Schema Validation & Fallback Logic**\n",
    "# Key Features:\n",
    "# 1. Pre-processing schema validation\n",
    "# 2. Critical column protection\n",
    "# 3. Graceful degradation for partial failures\n",
    "# 4. Comprehensive error logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9cb708b-55d5-478a-80b1-3e9cbf508293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7e3a1f-5689-40c5-b629-068cc7b9bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure error logging\n",
    "logging.basicConfig(\n",
    "    filename=f'data_pipeline_{datetime.now().strftime(\"%Y%m%d\")}.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('FinMarkPipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e968ec0-dfe9-4292-8467-571a17cbda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected schemas for each dataset\n",
    "SCHEMAS = {\n",
    "    'event_logs': {\n",
    "        'required': ['user_id', 'event_type', 'event_time', 'product_id', 'amount'],\n",
    "        'dtypes': {\n",
    "            'user_id': 'str',\n",
    "            'event_type': 'str',\n",
    "            'event_time': 'datetime64[ns]',\n",
    "            'product_id': 'str',\n",
    "            'amount': 'float'\n",
    "        }\n",
    "    },\n",
    "    'trend_report': {\n",
    "        'required': ['week', 'avg_users', 'sales_growth_rate'],\n",
    "        'dtypes': {\n",
    "            'week': 'datetime64[ns]',\n",
    "            'avg_users': 'int',\n",
    "            'sales_growth_rate': 'float'\n",
    "        }\n",
    "    },\n",
    "    'marketing_summary': {\n",
    "        'required': ['date', 'users_active', 'total_sales', 'new_customers'],\n",
    "        'dtypes': {\n",
    "            'date': 'datetime64[ns]',\n",
    "            'users_active': 'int',\n",
    "            'total_sales': 'float',\n",
    "            'new_customers': 'int'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66ea1984-999f-477d-9f40-a334a04faa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback defaults for critical columns\n",
    "FALLBACK_VALUES = {\n",
    "    'amount': 0.0,\n",
    "    'total_sales': 0.0,\n",
    "    'avg_users': 0,\n",
    "    'users_active': 0,\n",
    "    'new_customers': 0,\n",
    "    'sales_growth_rate': 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ea0b1f-a8f7-4fe1-8735-dd59733a1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save schemas for documentation\n",
    "with open('data_schemas.json', 'w') as f:\n",
    "    json.dump(SCHEMAS, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1788d019-8bb7-40a2-a529-f69ae351a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Validation & Repair Functions\n",
    "\n",
    "def validate_and_repair(df, dataset_name):\n",
    "    \"\"\"Validate schema and repair missing/corrupted columns\"\"\"\n",
    "    schema = SCHEMAS[dataset_name]\n",
    "    validation_report = {\n",
    "        'dataset': dataset_name,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'original_columns': list(df.columns),\n",
    "        'missing_columns': [],\n",
    "        'repaired_columns': [],\n",
    "        'type_issues': {},\n",
    "        'critical_failure': False\n",
    "    }\n",
    "        # 1. Check for missing columns\n",
    "    missing_cols = [col for col in schema['required'] if col not in df.columns]\n",
    "    validation_report['missing_columns'] = missing_cols\n",
    "    \n",
    "    if missing_cols:\n",
    "        logger.error(f\"Missing columns in {dataset_name}: {missing_cols}\")\n",
    "        for col in missing_cols:\n",
    "            if col in FALLBACK_VALUES:\n",
    "                df[col] = FALLBACK_VALUES[col]\n",
    "                logger.warning(f\"Created fallback column: {col} with default values\")\n",
    "                validation_report['repaired_columns'].append(col)\n",
    "            else:\n",
    "                logger.critical(f\"Unrecoverable missing column: {col}\")\n",
    "                validation_report['critical_failure'] = True\n",
    "    \n",
    "    # 2. Validate data types\n",
    "    for col, expected_type in schema['dtypes'].items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        current_type = str(df[col].dtype)\n",
    "        type_match = current_type == expected_type\n",
    "        \n",
    "        # Special handling for datetime columns\n",
    "        if expected_type == 'datetime64[ns]' and not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            type_match = False\n",
    "            \n",
    "        if not type_match:\n",
    "            validation_report['type_issues'][col] = {\n",
    "                'expected': expected_type,\n",
    "                'actual': current_type\n",
    "            }\n",
    "            logger.warning(f\"Type mismatch in {dataset_name}.{col}: {current_type} vs {expected_type}\")\n",
    "            \n",
    "            try:\n",
    "                # Handle datetime conversion\n",
    "                if expected_type == 'datetime64[ns]':\n",
    "                    df[col] = pd.to_datetime(df[col], format='ISO8601', errors='coerce')\n",
    "                    # Check if conversion created too many NaTs\n",
    "                    if df[col].isna().mean() > 0.5:\n",
    "                        logger.error(f\"Over 50% date conversion failures for {col}\")\n",
    "                        raise ValueError(\"Excessive date conversion errors\")\n",
    "                else:\n",
    "                    df[col] = df[col].astype(expected_type)\n",
    "                    \n",
    "                logger.info(f\"Successfully converted {col} to {expected_type}\")\n",
    "                validation_report['type_issues'][col]['repaired'] = True\n",
    "                \n",
    "            except (TypeError, ValueError) as e:\n",
    "                logger.error(f\"Type conversion failed for {col}: {str(e)}\")\n",
    "                if col in FALLBACK_VALUES:\n",
    "                    df[col] = FALLBACK_VALUES[col]\n",
    "                    logger.warning(f\"Reset corrupted column: {col} to default values\")\n",
    "                    validation_report['type_issues'][col]['fallback_used'] = True\n",
    "                else:\n",
    "                    validation_report['type_issues'][col]['repaired'] = False\n",
    "    \n",
    "    return df, validation_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce66e7d5-f860-415b-9d08-e819385cddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Core Processing Pipeline\n",
    "def process_dataset(file_path, dataset_name):\n",
    "    \"\"\"Load, validate, and clean dataset with error handling\"\"\"\n",
    "    logger.info(f\"Starting processing: {dataset_name}\")\n",
    "    validation_result = None\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        df = pd.read_csv(file_path)\n",
    "        logger.info(f\"Loaded {len(df)} records from {dataset_name}\")\n",
    "        \n",
    "        # Schema validation and repair\n",
    "        df, validation_result = validate_and_repair(df, dataset_name)\n",
    "        \n",
    "        if validation_result.get('critical_failure', False):\n",
    "            logger.error(f\"Critical validation failure for {dataset_name}. Aborting processing.\")\n",
    "            return None, validation_result\n",
    "        \n",
    "        # Dataset-specific cleaning\n",
    "        if dataset_name == 'event_logs':\n",
    "            df = clean_event_logs(df)\n",
    "        elif dataset_name == 'trend_report':\n",
    "            df = clean_trend_report(df)\n",
    "        elif dataset_name == 'marketing_summary':\n",
    "            df = clean_marketing_summary(df)\n",
    "            \n",
    "        logger.info(f\"Successfully processed {dataset_name}\")\n",
    "        return df, validation_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Fatal error processing {dataset_name}\")\n",
    "        return None, validation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00dd739f-0cc5-4ee7-aa54-b52220de985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset-Specific Cleaning Functions\n",
    "\n",
    "def clean_event_logs(df):\n",
    "    \"\"\"Cleaning logic for event logs\"\"\"\n",
    "    # Remove undefined columns\n",
    "    df = df[SCHEMAS['event_logs']['required']].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df['event_type'] = df['event_type'].fillna('unknown')\n",
    "    df['user_id'] = df['user_id'].fillna('UNK').astype(str)\n",
    "    \n",
    "    # Product/amount handling: Only relevant for orders\n",
    "    order_mask = df['event_type'].str.contains('order|purchase', case=False, na=False)\n",
    "    df.loc[order_mask, 'product_id'] = df.loc[order_mask, 'product_id'].fillna('PROD_UNK')\n",
    "    df.loc[order_mask, 'amount'] = df.loc[order_mask, 'amount'].fillna(0)\n",
    "    df.loc[~order_mask, 'product_id'] = df.loc[~order_mask, 'product_id'].fillna('N/A')\n",
    "    df.loc[~order_mask, 'amount'] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_trend_report(df):\n",
    "    \"\"\"Cleaning logic for trend report\"\"\"\n",
    "    # Keep only relevant columns\n",
    "    df = df[SCHEMAS['trend_report']['required']].copy()\n",
    "    \n",
    "    # Handle dates\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['week']):\n",
    "        df['week'] = pd.to_datetime(df['week'], format='ISO8601', errors='coerce')\n",
    "    \n",
    "    # Sort before forward-filling\n",
    "    df.sort_values('week', inplace=True)\n",
    "    \n",
    "    # Fill numeric metrics\n",
    "    df['avg_users'] = df['avg_users'].fillna(0)\n",
    "    df['sales_growth_rate'] = df['sales_growth_rate'].ffill().fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_marketing_summary(df):\n",
    "    \"\"\"Cleaning logic for marketing summary\"\"\"\n",
    "    # Keep relevant columns\n",
    "    df = df[SCHEMAS['marketing_summary']['required']].copy()\n",
    "    \n",
    "    # Date conversion\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "        df['date'] = pd.to_datetime(df['date'], format='ISO8601', errors='coerce')\n",
    "    \n",
    "    # Fill sequential data\n",
    "    df.sort_values('date', inplace=True)\n",
    "    for col in ['users_active', 'total_sales', 'new_customers']:\n",
    "        df[col] = df[col].ffill().fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1920cde-faab-4caf-b58c-071b5066e68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting FinMark Data Pipeline ===\n",
      "Execution timestamp: 2025-07-02T11:01:01.534031\n",
      "--------------------------------------\n",
      "\n",
      "Processing event_logs...\n",
      "✅ Successfully processed event_logs (2000 records)\n",
      "\n",
      "Processing trend_report...\n",
      "✅ Successfully processed trend_report (20 records)\n",
      "\n",
      "Processing marketing_summary...\n",
      "✅ Successfully processed marketing_summary (100 records)\n"
     ]
    }
   ],
   "source": [
    "## Pipeline Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets = {\n",
    "        'event_logs': 'event_logs.csv',\n",
    "        'trend_report': 'trend_report.csv',\n",
    "        'marketing_summary': 'marketing_summary.csv'\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    validation_reports = {}\n",
    "    \n",
    "    print(\"=== Starting FinMark Data Pipeline ===\")\n",
    "    print(f\"Execution timestamp: {datetime.now().isoformat()}\")\n",
    "    print(\"--------------------------------------\")\n",
    "    \n",
    "    for name, path in datasets.items():\n",
    "        print(f\"\\nProcessing {name}...\")\n",
    "        cleaned, report = process_dataset(path, name)\n",
    "        \n",
    "        validation_reports[name] = report\n",
    "        \n",
    "        if cleaned is not None:\n",
    "            results[name] = cleaned\n",
    "            # Save cleaned data\n",
    "            cleaned.to_csv(f'cleaned_{name}.csv', index=False)\n",
    "            print(f\"✅ Successfully processed {name} ({len(cleaned)} records)\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to process {name} - check error logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61ab5001-acfe-4326-b2bd-04b487e49abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation Summary ===\n",
      "\n",
      "Dataset: event_logs - PASSED\n",
      "Missing columns: 0\n",
      "Repaired columns: 0\n",
      "Type issues: 5\n",
      "\n",
      "Dataset: trend_report - PASSED\n",
      "Missing columns: 0\n",
      "Repaired columns: 0\n",
      "Type issues: 3\n",
      "\n",
      "Dataset: marketing_summary - PASSED\n",
      "Missing columns: 0\n",
      "Repaired columns: 0\n",
      "Type issues: 4\n",
      "\n",
      "Pipeline completed\n",
      "Successful datasets: 3/3\n",
      "Log file: data_pipeline_20250702.log\n"
     ]
    }
   ],
   "source": [
    "    # Generate validation summary\n",
    "    print(\"\\n=== Validation Summary ===\")\n",
    "    for name, report in validation_reports.items():\n",
    "        status = \"PASSED\" if name in results else \"FAILED\"\n",
    "        print(f\"\\nDataset: {name} - {status}\")\n",
    "        \n",
    "        if report:\n",
    "            print(f\"Missing columns: {len(report['missing_columns'])}\")\n",
    "            print(f\"Repaired columns: {len(report['repaired_columns'])}\")\n",
    "            print(f\"Type issues: {len(report.get('type_issues', {}))}\")\n",
    "    \n",
    "    print(\"\\nPipeline completed\")\n",
    "    print(f\"Successful datasets: {len(results)}/{len(datasets)}\")\n",
    "    print(f\"Log file: data_pipeline_{datetime.now().strftime('%Y%m%d')}.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc315993-1424-4071-b503-9524b75c6e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jonad\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jonad\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jonad\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jonad\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da814d-00a2-4e3e-9ca5-8e3fc580cbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
